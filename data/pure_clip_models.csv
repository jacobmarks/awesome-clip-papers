model_name,year,month,title,development,arxiv,github,open_source,license,model_card,open_clip_integration
CLIP,2021,2,Learning Transferable Visual Models From Natural Language Supervision,Simplified Contrastive Language-Image Pretraining,2103.00020,openai/CLIP/,Y,https://github.com/openai/CLIP/blob/main/LICENSE,https://github.com/openai/CLIP/blob/main/model-card.md,Y
ALIGN,2021,2,Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision,Extend from captions to noisy alt-text to avoid expensive filtering and post-processing,2102.05918,,Y,,https://huggingface.co/kakaobrain/align-base,N
CLOOB,2021,10,CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP,Avoid saturation of InfoNCE objective,2110.11316,ml-jku/cloob,Y,https://github.com/ml-jku/cloob?tab=readme-ov-file#license,,N
DeCLIP,2021,10,Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm,Data efficiency through supervision,2110.05208,Sense-GVT/DeCLIP,Y,https://github.com/Sense-GVT/DeCLIP?tab=readme-ov-file#license,,N
FILIP,2021,11,FILIP: Fine-grained Interactive Language-Image Pre-Training,"Adds token-wise maximum similarity bewteen visual and textual features for efficient and fine-grained semantic alignment ",2111.07783,,Y,,,N
DeFILIP,2022,3,"Democratizing Contrastive Language-Image Pre-training: A CLIP Benchmark of Data, Model, and Supervision",Combines DeCLIP and FILIP,2203.05796,Sense-GVT/DeCLIP,Y,https://github.com/Sense-GVT/DeCLIP?tab=readme-ov-file#license,,N
PyramidCLIP,2022,4,PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining,Relax assumption that image and metadata are in one-to-one correspondence,2204.14095,,N,,,N
KLITE,2022,4,K-LITE: Learning Transferable Visual Models with External Knowledge,Augment caption text with external knowledge,2204.09222,microsoft/klite,Y,https://github.com/microsoft/klite/blob/main/LICENSE,,N
CyCLIP,2022,5,CyCLIP: Cyclic Contrastive Language-Image Pretraining,Formalize and optimize for geometric consistency in image and text spaces,2205.14459,goel-shashank/CyCLIP,Y,https://github.com/goel-shashank/CyCLIP?tab=readme-ov-file#licenses,,N
FLIP,2022,12,Scaling Language-Image Pre-training via Masking,Masking images prior to encoding improves speed-accuracy trade-off for CLIP,2212.00794,facebookresearch/flip,Y,https://github.com/facebookresearch/flip/blob/main/LICENSE,,N
OpenCLIP,2022,12,Reproducible scaling laws for contrastive language-image learning,Open-source implementation of CLIP,2212.07143,mlfoundations/open_clip,Y,https://github.com/mlfoundations/open_clip/blob/main/LICENSE,https://github.com/mlfoundations/open_clip/blob/main/docs/PRETRAINED.md,Y
EVA-CLIP,2023,3,EVA-CLIP: Improved Training Techniques for CLIP at Scale,"Improved representation learning, optimization, and augmentation for faster training",2303.15389v1,baaivision/EVA/tree/master/EVA-CLIP,Y,,https://github.com/baaivision/EVA/tree/master/EVA-CLIP#model-card,Y
SigLIP,2023,3,Sigmoid Loss for Language Image Pre-Training,Sigmoid loss allows disentangling loss from batch size,2303.15343,google-research/big_vision,Y,https://github.com/google-research/big_vision?tab=readme-ov-file#license,,Y
CLIPA,2023,5,An Inverse Scaling Law for CLIP Training,"Insight into relationship between encoder size and training input sequence lengths leads to more efficient training  ",2305.07017,UCSC-VLAA/CLIPA,Y,https://github.com/UCSC-VLAA/CLIPA#license,,Y
MetaCLIP,2023,9,Demystifying CLIP Data,Rigorous study to reveal CLIP's data curation process,2309.16671,facebookresearch/MetaCLIP,Y,https://github.com/facebookresearch/MetaCLIP/blob/main/LICENSE,,Y
DFN,2023,11,Data Filtering Networks,A model trained on high-quality data can be used to filter massive online data employed to train the final CLIP model,2309.17425,,Y,https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378/blob/main/LICENSE,https://huggingface.co/apple/DFN5B-CLIP-ViT-H-14-378,Y
MobileCLIP,2023,11,MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training,Knowledge transfer from ensemble of strong models plus dataset reinforcement leads to 10-1000x improved learning efficiency,2311.17049,apple/ml-mobileclip,Y,https://github.com/apple/ml-mobileclip/blob/main/LICENSE,,